{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 Molmo: Image-to-Text Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685b970f70844cc99216bc790549b3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import torch\n",
    "\n",
    "login()\n",
    "\n",
    "HF_TOKEN = \"your_huggingface_token_here\"  # Replace with your Hugging Face token\n",
    "\n",
    "# This will be removed in the final version - LLaMA requires access from Meta\n",
    "hf_key = HF_TOKEN\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mohnah', 'nohmah', 'mohnoo', 'lohloh', 'looloo', 'mahnoh', 'moonoh', 'lohloo', 'nohnah', 'moomoo', 'puhkee', 'teepay', 'tuhkay', 'teekuh', 'tuhpuh', 'peekuh', 'peepay', 'paypay', 'kuhkuh', 'taypay']\n"
     ]
    }
   ],
   "source": [
    "possible_labels_s_r = [\n",
    "    \"looloo\", \"loolah\", \"looloh\", \"loonoo\", \"loonah\", \"loonoh\", \"loomoo\", \"loomah\", \"loomoh\",\n",
    "    \"lahloo\", \"lahlah\", \"lahloh\", \"lahnoo\", \"lahnah\", \"lahnoh\", \"lahmoo\", \"lahmah\", \"lahmoh\",\n",
    "    \"lohloo\", \"lohlah\", \"lohloh\", \"lohnoo\", \"lohnah\", \"lohnoh\", \"lohmoo\", \"lohmah\", \"lohmoh\",\n",
    "    \"nooloo\", \"noolah\", \"nooloh\", \"noonoo\", \"noonah\", \"noonoh\", \"noomoo\", \"noomah\", \"noomoh\",\n",
    "    \"nahloo\", \"nahlah\", \"nahloh\", \"nahnoo\", \"nahnah\", \"nahnoh\", \"nahmoo\", \"nahmah\", \"nahmoh\",\n",
    "    \"nohloo\", \"nohlah\", \"nohloh\", \"nohnoo\", \"nohnah\", \"nohnoh\", \"nohmoo\", \"nohmah\", \"nohmoh\",\n",
    "    \"mooloo\", \"moolah\", \"mooloh\", \"moonoo\", \"moonah\", \"moonoh\", \"moomoo\", \"moomah\", \"moomoh\",\n",
    "    \"mahloo\", \"mahlah\", \"mahloh\", \"mahnoo\", \"mahnah\", \"mahnoh\", \"mahmoo\", \"mahmah\", \"mahmoh\",\n",
    "    \"mohloo\", \"mohlah\", \"mohloh\", \"mohnoo\", \"mohnah\", \"mohnoh\", \"mohmoo\", \"mohmah\", \"mohmoh\"\n",
    "]\n",
    "\n",
    "possible_labels_p_nr = [\n",
    "    \"teetee\", \"teetuh\", \"teetay\", \"teekee\", \"teekuh\", \"teekay\", \"teepee\", \"teepuh\", \"teepay\",\n",
    "    \"tuhtee\", \"tuhtuh\", \"tuhtay\", \"tuhkee\", \"tuhkuh\", \"tuhkay\", \"tuhpee\", \"tuhpuh\", \"tuhpay\",\n",
    "    \"taytee\", \"taytuh\", \"taytay\", \"taykee\", \"taykuh\", \"taykay\", \"taypee\", \"taypuh\", \"taypay\",\n",
    "    \"keetee\", \"keetuh\", \"keetay\", \"keekee\", \"keekuh\", \"keekay\", \"keepee\", \"keepuh\", \"keepay\",\n",
    "    \"kuhtee\", \"kuhtuh\", \"kuhtay\", \"kuhkee\", \"kuhkuh\", \"kuhkay\", \"kuhpee\", \"kuhpuh\", \"kuhpay\",\n",
    "    \"kaytee\", \"kaytuh\", \"kaytay\", \"kaykee\", \"kaykuh\", \"kaykay\", \"kaypee\", \"kaypuh\", \"kaypay\",\n",
    "    \"peetee\", \"peetuh\", \"peetay\", \"peekee\", \"peekuh\", \"peekay\", \"peepee\", \"peepuh\", \"peepay\",\n",
    "    \"puhtee\", \"puhtuh\", \"puhtay\", \"puhkee\", \"puhkuh\", \"puhkay\", \"puhpee\", \"puhpuh\", \"puhpay\",\n",
    "    \"paytee\", \"paytuh\", \"paytay\", \"paykee\", \"paykuh\", \"paykay\", \"paypee\", \"paypuh\", \"paypay\"\n",
    "]\n",
    "\n",
    "possible_labels_s_r = random.sample(possible_labels_s_r, 10)\n",
    "possible_labels_p_nr = random.sample(possible_labels_p_nr, 10)\n",
    "\n",
    "final_list = possible_labels_s_r + possible_labels_p_nr\n",
    "\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f66baf105b4e6b930ca81ee0e27142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Classifying images 0-4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7078, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 0-4:  25%|██▌       | 1/4 [00:46<02:20, 46.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.5307, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8827\n",
      "Predicted text: looloo\n",
      "Token probabilities: [0.0708, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 0-4:  50%|█████     | 2/4 [01:20<01:18, 39.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6442, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9110\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.6504, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 0-4:  75%|███████▌  | 3/4 [01:56<00:37, 37.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5127, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8782\n",
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.2341, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 0-4: 100%|██████████| 4/4 [02:36<00:00, 39.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6092, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 4-8:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.1209, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 4-8:  25%|██▌       | 1/4 [00:35<01:46, 35.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.5014, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8754\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.6716, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 4-8:  50%|█████     | 2/4 [01:13<01:14, 37.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5649, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8912\n",
      "Predicted text: looloo\n",
      "Token probabilities: [0.1323, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 4-8:  75%|███████▌  | 3/4 [01:52<00:38, 38.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5985, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8996\n",
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.1966, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 4-8: 100%|██████████| 4/4 [02:26<00:00, 36.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.3253, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 8-12:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7944, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 8-12:  25%|██▌       | 1/4 [00:32<01:38, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.452, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8630\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.6504, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 8-12:  50%|█████     | 2/4 [01:06<01:06, 33.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.4256, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8564\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7396, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 8-12:  75%|███████▌  | 3/4 [01:50<00:38, 38.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.0789, 1.0, 0.5399, 1.0, 1.0]\n",
      "Mean confidence: 0.7237\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.5822, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 8-12: 100%|██████████| 4/4 [02:30<00:00, 37.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5352, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 12-16:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: looloo\n",
      "Token probabilities: [0.1074, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 12-16:  25%|██▌       | 1/4 [00:38<01:56, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.4548, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8637\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.8104, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 12-16:  50%|█████     | 2/4 [01:16<01:16, 38.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.706, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9265\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.6826, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 12-16:  75%|███████▌  | 3/4 [01:53<00:37, 37.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6483, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9121\n",
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.0808, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 12-16: 100%|██████████| 4/4 [02:29<00:00, 37.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5514, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 16-20:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7389, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 16-20:  25%|██▌       | 1/4 [00:36<01:49, 36.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.3548, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8387\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7922, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 16-20:  50%|█████     | 2/4 [01:13<01:13, 36.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.5017, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8754\n",
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.1782, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 16-20:  75%|███████▌  | 3/4 [01:47<00:35, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6198, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9050\n",
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.243, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 16-20: 100%|██████████| 4/4 [02:21<00:00, 35.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6282, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 20-24:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mahnoh\n",
      "Token probabilities: [0.2663, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 20-24:  25%|██▌       | 1/4 [00:36<01:49, 36.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6856, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9214\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.805, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 20-24:  50%|█████     | 2/4 [01:14<01:14, 37.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.4568, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8642\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.8343, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 20-24:  75%|███████▌  | 3/4 [01:46<00:35, 35.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.3936, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8484\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7358, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 20-24: 100%|██████████| 4/4 [02:21<00:00, 35.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.3776, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 24-28:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7158, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 24-28:  25%|██▌       | 1/4 [00:35<01:46, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6114, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9028\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.6666, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 24-28:  50%|█████     | 2/4 [01:14<01:14, 37.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.5831, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8958\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7211, 0.8945, 1.0, 1.0]\n",
      "Mean confidence: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 24-28:  75%|███████▌  | 3/4 [01:55<00:39, 39.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.7043, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9261\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.681, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 24-28: 100%|██████████| 4/4 [02:34<00:00, 38.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6472, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 28-32:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7563, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 28-32:  25%|██▌       | 1/4 [00:34<01:44, 34.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6383, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9096\n",
      "Predicted text: mohnah\n",
      "Token probabilities: [0.8376, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 28-32:  50%|█████     | 2/4 [01:09<01:09, 34.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.2899, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8225\n",
      "Predicted text: looloo\n",
      "Token probabilities: [0.1107, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 28-32:  75%|███████▌  | 3/4 [01:43<00:34, 34.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.4578, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8645\n",
      "Predicted text: looloo\n",
      "Token probabilities: [0.0954, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.7738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 28-32: 100%|██████████| 4/4 [02:19<00:00, 34.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.4737, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 32-34:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: mohnah\n",
      "Token probabilities: [0.7995, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 32-34:  50%|█████     | 1/2 [00:36<00:36, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: tuhkay\n",
      "Token probabilities: [0.291, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.8227\n",
      "Predicted text: nohmah\n",
      "Token probabilities: [0.075, 0.4448, 1.0, 1.0]\n",
      "Mean confidence: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying images 32-34: 100%|██████████| 2/2 [01:13<00:00, 36.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text: teepay\n",
      "Token probabilities: [0.6511, 1.0, 1.0, 1.0]\n",
      "Mean confidence: 0.9128\n",
      "Classification complete. Results saved to CSV files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class ImageTextMatcher:\n",
    "    def __init__(self, image_folder=\"images\"):\n",
    "        \"\"\"\n",
    "        Initialize the analyzer with the path to the image folder.\n",
    "        \n",
    "        Args:\n",
    "            image_folder: Path to the folder containing images\n",
    "        \"\"\"\n",
    "        self.image_folder = image_folder\n",
    "        self.results = []\n",
    "        self.models = {}\n",
    "        \n",
    "    def load_model(self, model_name):\n",
    "        if model_name == 'molmo':\n",
    "            processor = AutoProcessor.from_pretrained(\n",
    "                'allenai/Molmo-7B-D-0924',\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto')\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                'allenai/Molmo-7B-D-0924',\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto')\n",
    "            self.models['molmo'] = {'model': model, 'processor': processor}\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}, Model not Found.\")\n",
    "    \n",
    "    def classify_image(self, image_path, possible_labels):\n",
    "        \"\"\"\n",
    "        Classify the image into one of the predefined classes.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "        \n",
    "        Returns:\n",
    "            Predicted class name\n",
    "        \"\"\"\n",
    "        model_info = self.models['molmo']\n",
    "        model = model_info['model']\n",
    "        processor = model_info['processor']\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Prompt setup\n",
    "        prompt = (\n",
    "            f\"You are given an image for which you need to assign a label. Use one of the following labels: {possible_labels}. Only respond with the label.\"\n",
    "        )\n",
    "        \n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=prompt\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate with output scores to compute probabilities\n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type=\"cuda\", enabled=True, dtype=torch.float32):\n",
    "                outputs = model.generate_from_batch(\n",
    "                    inputs,\n",
    "                    GenerationConfig(max_new_tokens=5, stop_strings=\"<|endoftext|>\"),\n",
    "                    tokenizer=processor.tokenizer,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    top_k=40\n",
    "                )\n",
    "\n",
    "                generated_token_ids = outputs.sequences[0][inputs['input_ids'].size(1):]  # Only new tokens\n",
    "                scores = outputs.scores  # Logits for each new token\n",
    "\n",
    "                token_probs = []\n",
    "                for i, token_id in enumerate(generated_token_ids):\n",
    "                    logits = scores[i]  # Logits for i-th token\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    token_prob = probs[0, token_id]\n",
    "                    token_probs.append(token_prob.item())\n",
    "\n",
    "                # Final predicted text (after decoding tokens)\n",
    "                predicted_text = processor.tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "                # Confidence score (mean of token probabilities)\n",
    "                confidence_score = sum(token_probs) / len(token_probs)\n",
    "\n",
    "                print(f\"Predicted text: {predicted_text}\")\n",
    "                print(f\"Token probabilities: {[round(p, 4) for p in token_probs]}\")\n",
    "                print(f\"Mean confidence: {confidence_score:.4f}\")\n",
    "\n",
    "                return predicted_text, confidence_score\n",
    "            \n",
    "    def prepare_dataset_for_classification(self, image_paths=None):\n",
    "        \"\"\"\n",
    "        Prepare dataset for image classification.\n",
    "        \n",
    "        Args:\n",
    "            image_paths: List of paths to images (if None, scan the image folder)\n",
    "            \n",
    "        Returns:\n",
    "            List of image paths\n",
    "        \"\"\"\n",
    "        # If image paths are not provided, scan the image folder\n",
    "        if image_paths is None:\n",
    "            image_paths = []\n",
    "            for filename in os.listdir(self.image_folder):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    image_paths.append(os.path.join(self.image_folder, filename))\n",
    "        \n",
    "        return image_paths\n",
    "    \n",
    "    def classify_dataset(self, model_name, image_paths, possible_labels_s_r, possible_labels_p_nr):\n",
    "        \"\"\"\n",
    "        Classify a dataset of images using S-R and P-NR labels and calculate confidence scores.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to use\n",
    "            image_paths: List of image paths to classify\n",
    "            possible_labels_s_r: Labels for sonorant+rounded pseudowords\n",
    "            possible_labels_p_nr: Labels for plosief+non-rounded pseudowords\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with classification results\n",
    "        \"\"\"\n",
    "        if model_name not in self.models:\n",
    "            self.load_model(model_name)\n",
    "        \n",
    "        classification_results = []\n",
    "\n",
    "        # Classify images in batches of 4\n",
    "        batch_size = 4\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch = image_paths[i:i+batch_size]\n",
    "            \n",
    "            for image_path in tqdm(batch, desc=f\"Classifying images {i}-{i+len(batch)}\"):\n",
    "                try:\n",
    "                    # First classify with S-R labels\n",
    "                    predicted_class_s_r, score_s_r = self.classify_image(image_path, possible_labels_s_r)\n",
    "                    \n",
    "                    # Then classify with P-NR labels\n",
    "                    predicted_class_p_nr, score_p_nr = self.classify_image(image_path, possible_labels_p_nr)\n",
    "\n",
    "                    filename = os.path.basename(image_path)\n",
    "                    image_type = 'Unknown'\n",
    "                    if 'curved' in filename.lower():\n",
    "                        image_type = 'Curved'\n",
    "                    elif 'jagged' in filename.lower():\n",
    "                        image_type = 'Jagged'\n",
    "                                \n",
    "                    # Store result with confidence scores\n",
    "                    classification_results.append({\n",
    "                        'image_path': image_path,\n",
    "                        'image_filename': os.path.basename(image_path),\n",
    "                        'image_type': image_type,\n",
    "                        'predicted_class_s_r': predicted_class_s_r,\n",
    "                        'score_s_r': score_s_r,\n",
    "                        'predicted_class_p_nr': predicted_class_p_nr,\n",
    "                        'score_p_nr': score_p_nr\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error classifying {image_path}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            # Clear CUDA cache between batches\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        return pd.DataFrame(classification_results)\n",
    "    \n",
    "    def analyze_classification_results(self, results_df):\n",
    "        \"\"\"\n",
    "        Analyze the classification results.\n",
    "        \n",
    "        Args:\n",
    "            results_df: DataFrame with classification results\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with classification metrics\n",
    "        \"\"\"\n",
    "        # Extract and analyze the scores for S-R and P-NR words\n",
    "        curved_scores_s_r = results_df[results_df['image_type'] == 'Curved']['score_s_r']\n",
    "        jagged_scores_s_r = results_df[results_df['image_type'] == 'Jagged']['score_s_r']\n",
    "        \n",
    "        curved_scores_p_nr = results_df[results_df['image_type'] == 'Curved']['score_p_nr']\n",
    "        jagged_scores_p_nr = results_df[results_df['image_type'] == 'Jagged']['score_p_nr']\n",
    "        \n",
    "        # Compare average scores\n",
    "        analysis_results = {\n",
    "            'avg_score_s_r_curved': curved_scores_s_r.mean(),\n",
    "            'avg_score_s_r_jagged': jagged_scores_s_r.mean(),\n",
    "            'avg_score_p_nr_curved': curved_scores_p_nr.mean(),\n",
    "            'avg_score_p_nr_jagged': jagged_scores_p_nr.mean()\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame([analysis_results])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    matcher = ImageTextMatcher(image_folder=\"images\")\n",
    "    image_paths = matcher.prepare_dataset_for_classification()\n",
    "    classification_results = matcher.classify_dataset(\"molmo\", image_paths, possible_labels_s_r, possible_labels_p_nr)\n",
    "\n",
    "    ## Change folder to [1,2,3]\n",
    "    # Save results\n",
    "    classification_results.to_csv(\"10/image_classifications.csv\", index=False)\n",
    "    \n",
    "    # Save metrics\n",
    "    classification_metrics = matcher.analyze_classification_results(classification_results)\n",
    "    classification_metrics.to_csv(\"10/classification_metrics.csv\", index=False)\n",
    "    \n",
    "    print(\"Classification complete. Results saved to CSV files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
